## Insurance claims prediction

You can use the [editor on GitHub](https://github.com/Roni-N/Insurance-claims-prediction/edit/gh-pages/index.md) to maintain and preview the content for your website in Markdown files.

Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.

### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for


------------------------------------------------------------


# Insurance claims prediction project.
This project is a supervised learning classification task based on structured data.
In this project we will develop an Insurance claim prediction model using different data sources.

An insurance claim prediction model could be a valuable and powerful tool. 
By using this tool, insurance companies can minimize risks, provide adaptive pricing premiums, streamline work processes, reduce costs and maximize profits.

In this project, we will use the CRISP-DM methodology. 
CRISP-DM is a popular workflow, including six steps (fig 1). In each step, we will perform different techniques that will help us to build and develop an efficient model.

------------------------------------------------------------

## Section 1: Business Understanding
In this section, we will learn about the business world (the insurance industry), and we will define the business problem and our project objectives.
Pic 1 and link


------------------------------------------------------------
## Section 2: Data Understanding
************************************************************

### 2.1 Data integration 
In this section, firstly, we will explore the available data (features) from the different sources. 
We will integrate the data into a unified dataset, and we will produce the target variable.
Pic 1 and link
************************************************************

### 2.2 Exploratory Data Analysis (EDA)
In the next step, we will perform Exploratory Data Analysis (EDA) to perform initial investigations, discover patterns, spot anomalies, test hypotheses, and check assumptions with the help of summary statistics and graphical representations.
Pic 1 and link

------------------------------------------------------------
## Section 3: Data preparation 
### 3.1 Feature Engineering:
In this section, we will use different methods and techniques to transform and extract raw data into features that better represent the underlying problem to the predictive model.
The main steps include:
Missing Data Imputation
Categorical Features Encoding
Transformations
Discretisations
Outliers Handling
Features Scaling
Engineering mixed Features
Pic 1 and link
************************************************************
### 3.2 Feature Selection: 
In the next step, we will perform Feature selection methods to reduce (removing non-informative or redundant) the number of input variables to those that we believe to be most useful to a model in order to predict the target variable. 
We have three main approaches to handle this step:
Filter methods (variance and statistical test, correlations, univariate selection)
Wrapper methods (forward / backward selection, exhaustive search)
Embedded methods (lasso, tree importance)
Pic 1 and link
************************************************************
### 3.3 Imbalance Data: 
The final step in this section will be Imbalanced data handling. In our data set, the number of observations per class is not equally distributed. This issue cloud encountered a significant drawback of the performance attainable by most standard classifier learning algorithms.
We have three main approaches to handle this step:
Undersampling (Fixed and Cleaning methods)
Oversampling ( sample extraction and sample generation methods)
Ensemble Method (data level, cost-sensitive, ensemble algorithms)
Pic 1 and link
************************************************************

------------------------------------------------------------
## Section 4: Machine Learning Algorithms 
In this section, we will train different machine learning classification algorithms.
We will train models like:
Logistic regression 
SVM
Naive Bayes classifier  
Random Forest
KNN classifier 
And more

------------------------------------------------------------
## Section 5: Evaluation and Hyperparameters Tuning 
In this section, we will evaluate the model's performance by performance metrics.
And we will perform Hyperparameters Tuning methods (i.e. choosing a set of optimal hyperparameters for a learning algorithm) to improve model performance.
In this step, we will use popular methods as:
Cross-validation
Grid search
And more

------------------------------------------------------------
```markdown
Syntax highlighted code block



# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/Roni-N/Insurance-claims-prediction/settings/pages). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://support.github.com/contact) and weâ€™ll help you sort it out.
